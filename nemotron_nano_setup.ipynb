{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Nemotron Nano 9B v2 NIM Setup and Inference\n",
        "\n",
        "This notebook will guide you through:\n",
        "1. Setting up your NGC API key\n",
        "2. Logging into the NVIDIA Container Registry\n",
        "3. Pulling and running the Nemotron Nano 9B v2 NIM container\n",
        "4. Performing inference against the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Enter Your NGC API Key\n",
        "\n",
        "Get your API key from: https://build.nvidia.com/nvidia/nvidia-nemotron-nano-9b-v2/deploy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Prompt for API key securely\n",
        "NGC_API_KEY = getpass.getpass(\"Enter your NGC API Key: \")\n",
        "os.environ['NGC_API_KEY'] = NGC_API_KEY\n",
        "\n",
        "print(\"✓ API Key saved to environment\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Login to NVIDIA Container Registry\n",
        "\n",
        "This will authenticate Docker with the NVIDIA Container Registry (nvcr.io)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Docker login to nvcr.io\n",
        "print(\"Logging into nvcr.io...\")\n",
        "\n",
        "login_process = subprocess.Popen(\n",
        "    ['docker', 'login', 'nvcr.io', '-u', '$oauthtoken', '--password-stdin'],\n",
        "    stdin=subprocess.PIPE,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "stdout, stderr = login_process.communicate(input=NGC_API_KEY)\n",
        "\n",
        "if login_process.returncode == 0:\n",
        "    print(\"✓ Successfully logged into nvcr.io\")\n",
        "    print(stdout)\n",
        "else:\n",
        "    print(\"✗ Login failed\")\n",
        "    print(stderr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Setup Local Cache and Pull Container\n",
        "\n",
        "This creates a local cache directory and pulls the NIM container image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup local cache directory\n",
        "LOCAL_NIM_CACHE = os.path.expanduser(\"~/.cache/nim\")\n",
        "os.makedirs(LOCAL_NIM_CACHE, exist_ok=True)\n",
        "print(f\"✓ Created cache directory: {LOCAL_NIM_CACHE}\")\n",
        "\n",
        "# Pull the container image\n",
        "print(\"\\nPulling container image (this may take several minutes)...\")\n",
        "pull_result = subprocess.run(\n",
        "    ['docker', 'pull', 'nvcr.io/nim/nvidia/nvidia-nemotron-nano-9b-v2:latest'],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "if pull_result.returncode == 0:\n",
        "    print(\"✓ Container image pulled successfully\")\n",
        "else:\n",
        "    print(\"✗ Failed to pull container\")\n",
        "    print(pull_result.stderr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run the NIM Container\n",
        "\n",
        "**Note:** This will start the container in the background. The model may take a few minutes to load and be ready for inference.\n",
        "\n",
        "**Important:** Make sure you have:\n",
        "- NVIDIA GPU with appropriate drivers\n",
        "- Docker with GPU support (nvidia-docker)\n",
        "- Sufficient disk space (~20GB+)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get current user ID\n",
        "user_id = os.getuid()\n",
        "\n",
        "# Check if container is already running\n",
        "check_container = subprocess.run(\n",
        "    ['docker', 'ps', '--filter', 'ancestor=nvcr.io/nim/nvidia/nvidia-nemotron-nano-9b-v2:latest', '-q'],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "if check_container.stdout.strip():\n",
        "    print(\"⚠ Container is already running\")\n",
        "    container_id = check_container.stdout.strip().split('\\n')[0]\n",
        "    print(f\"Container ID: {container_id}\")\n",
        "else:\n",
        "    print(\"Starting NIM container...\")\n",
        "    print(\"This will run in detached mode. Monitor logs with: docker logs -f <container_id>\")\n",
        "    \n",
        "    run_result = subprocess.Popen(\n",
        "        [\n",
        "            'docker', 'run', '-d',\n",
        "            '--gpus', 'all',\n",
        "            '--shm-size=16GB',\n",
        "            '-e', f'NGC_API_KEY={NGC_API_KEY}',\n",
        "            '-v', f'{LOCAL_NIM_CACHE}:/opt/nim/.cache',\n",
        "            '-u', str(user_id),\n",
        "            '-p', '8000:8000',\n",
        "            'nvcr.io/nim/nvidia/nvidia-nemotron-nano-9b-v2:latest'\n",
        "        ],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True\n",
        "    )\n",
        "    \n",
        "    stdout, stderr = run_result.communicate()\n",
        "    \n",
        "    if run_result.returncode == 0:\n",
        "        container_id = stdout.strip()\n",
        "        print(f\"✓ Container started successfully\")\n",
        "        print(f\"Container ID: {container_id}\")\n",
        "        print(\"\\nWaiting for model to load (this may take 2-5 minutes)...\")\n",
        "    else:\n",
        "        print(\"✗ Failed to start container\")\n",
        "        print(stderr)\n",
        "        container_id = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Wait for Model to be Ready\n",
        "\n",
        "This cell will poll the health endpoint until the model is ready to accept requests.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Wait for the service to be ready\n",
        "max_retries = 60  # 5 minutes max\n",
        "retry_interval = 5  # seconds\n",
        "\n",
        "print(\"Checking if service is ready...\")\n",
        "for i in range(max_retries):\n",
        "    try:\n",
        "        response = requests.get('http://localhost:8000/v1/health/ready', timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"\\n✓ Service is ready! (took {i * retry_interval} seconds)\")\n",
        "            # Also check models endpoint\n",
        "            models_response = requests.get('http://localhost:8000/v1/models', timeout=5)\n",
        "            if models_response.status_code == 200:\n",
        "                print(f\"Available models: {models_response.json()}\")\n",
        "            break\n",
        "    except requests.exceptions.RequestException:\n",
        "        pass\n",
        "    \n",
        "    if i % 6 == 0:  # Print every 30 seconds\n",
        "        print(f\"Waiting... ({i * retry_interval}s elapsed)\")\n",
        "    \n",
        "    time.sleep(retry_interval)\n",
        "else:\n",
        "    print(\"\\n✗ Service did not become ready in time. Check container logs with:\")\n",
        "    print(f\"docker logs {container_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Perform Inference\n",
        "\n",
        "Now let's test the model with a sample question!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform inference\n",
        "url = 'http://localhost:8000/v1/chat/completions'\n",
        "\n",
        "payload = {\n",
        "    \"model\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Which number is larger, 9.11 or 9.8?\"}\n",
        "    ],\n",
        "    \"max_tokens\": 2048,  # Increased default for better responses\n",
        "    \"temperature\": 0.6,\n",
        "    \"top_p\": 0.95\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    'accept': 'application/json',\n",
        "    'Content-Type': 'application/json'\n",
        "}\n",
        "\n",
        "print(\"Sending inference request...\\n\")\n",
        "response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(\"✓ Inference successful!\\n\")\n",
        "    print(\"Response:\")\n",
        "    print(json.dumps(result, indent=2))\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Model's Answer:\")\n",
        "    print(result['choices'][0]['message']['content'])\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(f\"✗ Inference failed with status code: {response.status_code}\")\n",
        "    print(response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Try Your Own Questions!\n",
        "\n",
        "Use this cell to experiment with your own prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom inference function\n",
        "def ask_nemotron(question, max_tokens=2048, temperature=0.6, top_p=0.95):\n",
        "    url = 'http://localhost:8000/v1/chat/completions'\n",
        "    \n",
        "    payload = {\n",
        "        \"model\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ],\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p\n",
        "    }\n",
        "    \n",
        "    response = requests.post(url, json=payload, headers={'Content-Type': 'application/json'})\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result['choices'][0]['message']['content']\n",
        "    else:\n",
        "        return f\"Error: {response.status_code} - {response.text}\"\n",
        "\n",
        "# Try it out!\n",
        "question = \"What are the three laws of robotics?\"\n",
        "answer = ask_nemotron(question)\n",
        "print(f\"Q: {question}\\n\")\n",
        "print(f\"A: {answer}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Thinking Mode - A Unique Capability!\n",
        "\n",
        "**Important:** Nemotron Nano 9B v2 has a special thinking capability that allows it to reason through problems before answering.\n",
        "\n",
        "- **By default, the model thinks** unless you explicitly disable it\n",
        "- To **enable thinking explicitly**, add `\"/think\"` to the system message\n",
        "- To **disable thinking**, add `\"/no_think\"` to the front of your prompt\n",
        "- You can control thinking with `min_thinking_tokens` and `max_thinking_tokens` parameters\n",
        "\n",
        "Let's explore this powerful feature!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Example 1: Thinking Mode (Explicit)\n",
        "# With /think in system message, the model will reason through the problem\n",
        "\n",
        "url = 'http://localhost:8000/v1/chat/completions'\n",
        "\n",
        "payload = {\n",
        "    \"model\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"/think\"},\n",
        "        {\"role\": \"user\", \"content\": \"If a train travels 120 miles in 2 hours, then slows down and travels 90 miles in 3 hours, what is its average speed for the entire journey?\"}\n",
        "    ],\n",
        "    \"temperature\": 0.6,   \n",
        "    \"top_p\": 0.95,\n",
        "    \"max_tokens\": 2048,\n",
        "    \"min_thinking_tokens\": 512,\n",
        "    \"max_thinking_tokens\": 2048,\n",
        "    \"stream\": False\n",
        "}\n",
        "\n",
        "print(\"🧠 THINKING MODE - Asking the model to solve a problem...\\n\")\n",
        "response = requests.post(url, json=payload, headers={'Content-Type': 'application/json'})\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Model's Response (with internal reasoning):\")\n",
        "    print(\"=\" * 80)\n",
        "    print(result['choices'][0]['message']['content'])\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Usage stats: {json.dumps(result.get('usage', {}), indent=2)}\")\n",
        "    print(\"=\" * 80)\n",
        "else:\n",
        "    print(f\"Error: {response.status_code} - {response.text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Example 2: No-Thinking Mode\n",
        "# With /no_think prefix, the model responds directly without reasoning\n",
        "\n",
        "payload_no_think = {\n",
        "    \"model\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"/no_think If a train travels 120 miles in 2 hours, then slows down and travels 90 miles in 3 hours, what is its average speed for the entire journey?\"}\n",
        "    ],\n",
        "    \"temperature\": 0.6,   \n",
        "    \"top_p\": 0.95,\n",
        "    \"max_tokens\": 2048,\n",
        "    \"stream\": False\n",
        "}\n",
        "\n",
        "print(\"💨 NO-THINKING MODE - Direct response without reasoning...\\n\")\n",
        "response = requests.post(url, json=payload_no_think, headers={'Content-Type': 'application/json'})\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Model's Response (direct, no reasoning):\")\n",
        "    print(\"=\" * 80)\n",
        "    print(result['choices'][0]['message']['content'])\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Usage stats: {json.dumps(result.get('usage', {}), indent=2)}\")\n",
        "    print(\"=\" * 80)\n",
        "else:\n",
        "    print(f\"Error: {response.status_code} - {response.text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Example 3: Side-by-Side Comparison\n",
        "# Let's compare thinking vs no-thinking on a challenging problem\n",
        "\n",
        "def compare_thinking_modes(question):\n",
        "    \"\"\"Compare model responses with and without thinking\"\"\"\n",
        "    \n",
        "    # With thinking\n",
        "    payload_think = {\n",
        "        \"model\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"/think\"},\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ],\n",
        "        \"temperature\": 0.6,\n",
        "        \"top_p\": 0.95,\n",
        "        \"max_tokens\": 2048,\n",
        "        \"min_thinking_tokens\": 512,\n",
        "        \"max_thinking_tokens\": 2048\n",
        "    }\n",
        "    \n",
        "    # Without thinking\n",
        "    payload_no_think = {\n",
        "        \"model\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": f\"/no_think {question}\"}\n",
        "        ],\n",
        "        \"temperature\": 0.6,\n",
        "        \"top_p\": 0.95,\n",
        "        \"max_tokens\": 2048\n",
        "    }\n",
        "    \n",
        "    print(f\"Question: {question}\\n\")\n",
        "    print(\"🧠 WITH THINKING:\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    response = requests.post(url, json=payload_think, headers={'Content-Type': 'application/json'})\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        print(result['choices'][0]['message']['content'][:500] + \"...\" if len(result['choices'][0]['message']['content']) > 500 else result['choices'][0]['message']['content'])\n",
        "        think_tokens = result.get('usage', {})\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        think_tokens = {}\n",
        "    \n",
        "    print(\"\\n\\n💨 WITHOUT THINKING:\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    response = requests.post(url, json=payload_no_think, headers={'Content-Type': 'application/json'})\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        print(result['choices'][0]['message']['content'][:500] + \"...\" if len(result['choices'][0]['message']['content']) > 500 else result['choices'][0]['message']['content'])\n",
        "        no_think_tokens = result.get('usage', {})\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        no_think_tokens = {}\n",
        "    \n",
        "    print(\"\\n\\n📊 TOKEN COMPARISON:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"With thinking:    {json.dumps(think_tokens, indent=2)}\")\n",
        "    print(f\"Without thinking: {json.dumps(no_think_tokens, indent=2)}\")\n",
        "\n",
        "# Test with a logic puzzle\n",
        "compare_thinking_modes(\n",
        "    \"A farmer has 17 sheep, and all but 9 die. How many sheep are left?\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Example 4: Streaming with Thinking Tokens\n",
        "# You can also stream the response to see tokens as they're generated\n",
        "\n",
        "print(\"🌊 STREAMING MODE - Watch the model think and respond in real-time!\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "payload_stream = {\n",
        "    \"model\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"/think\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explain the concept of recursion with a simple example.\"}\n",
        "    ],\n",
        "    \"temperature\": 0.6,   \n",
        "    \"top_p\": 0.95,\n",
        "    \"max_tokens\": 2048,\n",
        "    \"min_thinking_tokens\": 256,\n",
        "    \"max_thinking_tokens\": 1024,\n",
        "    \"stream\": True\n",
        "}\n",
        "\n",
        "response = requests.post(\n",
        "    url, \n",
        "    json=payload_stream, \n",
        "    headers={'Content-Type': 'application/json'},\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    full_response = \"\"\n",
        "    for line in response.iter_lines():\n",
        "        if line:\n",
        "            line = line.decode('utf-8')\n",
        "            if line.startswith('data: '):\n",
        "                data = line[6:]  # Remove 'data: ' prefix\n",
        "                if data.strip() == '[DONE]':\n",
        "                    break\n",
        "                try:\n",
        "                    chunk = json.loads(data)\n",
        "                    if 'choices' in chunk and len(chunk['choices']) > 0:\n",
        "                        delta = chunk['choices'][0].get('delta', {})\n",
        "                        content = delta.get('content', '')\n",
        "                        if content:\n",
        "                            print(content, end='', flush=True)\n",
        "                            full_response += content\n",
        "                except json.JSONDecodeError:\n",
        "                    pass\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"✓ Streaming complete!\")\n",
        "else:\n",
        "    print(f\"Error: {response.status_code} - {response.text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Example 5: Custom Thinking Function\n",
        "# Here's a reusable function for easy experimentation with thinking parameters\n",
        "\n",
        "def ask_with_thinking(\n",
        "    question, \n",
        "    enable_thinking=True,\n",
        "    max_tokens=2048,\n",
        "    min_thinking_tokens=512,\n",
        "    max_thinking_tokens=2048,\n",
        "    temperature=0.6,\n",
        "    top_p=0.95,\n",
        "    stream=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Ask the model a question with optional thinking mode.\n",
        "    \n",
        "    Args:\n",
        "        question: The question to ask\n",
        "        enable_thinking: Whether to enable thinking mode (default: True)\n",
        "        max_tokens: Maximum tokens for the response\n",
        "        min_thinking_tokens: Minimum thinking tokens (if thinking enabled)\n",
        "        max_thinking_tokens: Maximum thinking tokens (if thinking enabled)\n",
        "        temperature: Sampling temperature\n",
        "        top_p: Nucleus sampling parameter\n",
        "        stream: Whether to stream the response\n",
        "    \"\"\"\n",
        "    url = 'http://localhost:8000/v1/chat/completions'\n",
        "    \n",
        "    if enable_thinking:\n",
        "        payload = {\n",
        "            \"model\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"/think\"},\n",
        "                {\"role\": \"user\", \"content\": question}\n",
        "            ],\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": top_p,\n",
        "            \"max_tokens\": max_tokens,\n",
        "            \"min_thinking_tokens\": min_thinking_tokens,\n",
        "            \"max_thinking_tokens\": max_thinking_tokens,\n",
        "            \"stream\": stream\n",
        "        }\n",
        "    else:\n",
        "        payload = {\n",
        "            \"model\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": f\"/no_think {question}\"}\n",
        "            ],\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": top_p,\n",
        "            \"max_tokens\": max_tokens,\n",
        "            \"stream\": stream\n",
        "        }\n",
        "    \n",
        "    response = requests.post(url, json=payload, headers={'Content-Type': 'application/json'})\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result['choices'][0]['message']['content'], result.get('usage', {})\n",
        "    else:\n",
        "        return f\"Error: {response.status_code} - {response.text}\", {}\n",
        "\n",
        "# Test it out!\n",
        "question = \"Write a Python function that finds the longest palindrome in a string.\"\n",
        "answer, usage = ask_with_thinking(question, enable_thinking=True)\n",
        "\n",
        "print(f\"Q: {question}\\n\")\n",
        "print(f\"A: {answer}\\n\")\n",
        "print(f\"Token usage: {json.dumps(usage, indent=2)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Cleanup (Optional)\n",
        "\n",
        "When you're done, you can stop and remove the container.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop the container\n",
        "if 'container_id' in locals() and container_id:\n",
        "    print(f\"Stopping container {container_id}...\")\n",
        "    subprocess.run(['docker', 'stop', container_id])\n",
        "    print(\"✓ Container stopped\")\n",
        "else:\n",
        "    # Try to find and stop any running nemotron containers\n",
        "    result = subprocess.run(\n",
        "        ['docker', 'ps', '--filter', 'ancestor=nvcr.io/nim/nvidia/nvidia-nemotron-nano-9b-v2:latest', '-q'],\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    if result.stdout.strip():\n",
        "        containers = result.stdout.strip().split('\\n')\n",
        "        for cid in containers:\n",
        "            print(f\"Stopping container {cid}...\")\n",
        "            subprocess.run(['docker', 'stop', cid])\n",
        "        print(\"✓ All containers stopped\")\n",
        "    else:\n",
        "        print(\"No running containers found\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

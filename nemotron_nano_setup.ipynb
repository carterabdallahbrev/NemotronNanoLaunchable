{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Nemotron Nano 9B v2 NIM Setup and Inference\n",
        "\n",
        "This notebook will guide you through:\n",
        "1. Setting up your NGC API key\n",
        "2. Logging into the NVIDIA Container Registry\n",
        "3. Pulling and running the Nemotron Nano 9B v2 NIM container\n",
        "4. Performing inference against the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Enter Your NGC API Key\n",
        "\n",
        "Get your API key from: https://build.nvidia.com/nvidia/nvidia-nemotron-nano-9b-v2/deploy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Prompt for API key securely\n",
        "NGC_API_KEY = getpass.getpass(\"Enter your NGC API Key: \")\n",
        "os.environ['NGC_API_KEY'] = NGC_API_KEY\n",
        "\n",
        "print(\"✓ API Key saved to environment\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Login to NVIDIA Container Registry\n",
        "\n",
        "This will authenticate Docker with the NVIDIA Container Registry (nvcr.io)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Docker login to nvcr.io\n",
        "print(\"Logging into nvcr.io...\")\n",
        "\n",
        "login_process = subprocess.Popen(\n",
        "    ['docker', 'login', 'nvcr.io', '-u', '$oauthtoken', '--password-stdin'],\n",
        "    stdin=subprocess.PIPE,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "stdout, stderr = login_process.communicate(input=NGC_API_KEY)\n",
        "\n",
        "if login_process.returncode == 0:\n",
        "    print(\"✓ Successfully logged into nvcr.io\")\n",
        "    print(stdout)\n",
        "else:\n",
        "    print(\"✗ Login failed\")\n",
        "    print(stderr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Setup Local Cache and Pull Container\n",
        "\n",
        "This creates a local cache directory and pulls the NIM container image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup local cache directory\n",
        "LOCAL_NIM_CACHE = os.path.expanduser(\"~/.cache/nim\")\n",
        "os.makedirs(LOCAL_NIM_CACHE, exist_ok=True)\n",
        "print(f\"✓ Created cache directory: {LOCAL_NIM_CACHE}\")\n",
        "\n",
        "# Pull the container image\n",
        "print(\"\\nPulling container image (this may take several minutes)...\")\n",
        "pull_result = subprocess.run(\n",
        "    ['docker', 'pull', 'nvcr.io/nim/nvidia/nvidia-nemotron-nano-9b-v2:latest'],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "if pull_result.returncode == 0:\n",
        "    print(\"✓ Container image pulled successfully\")\n",
        "else:\n",
        "    print(\"✗ Failed to pull container\")\n",
        "    print(pull_result.stderr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run the NIM Container\n",
        "\n",
        "**Note:** This will start the container in the background. The model may take a few minutes to load and be ready for inference.\n",
        "\n",
        "**Important:** Make sure you have:\n",
        "- NVIDIA GPU with appropriate drivers\n",
        "- Docker with GPU support (nvidia-docker)\n",
        "- Sufficient disk space (~20GB+)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get current user ID\n",
        "user_id = os.getuid()\n",
        "\n",
        "# Check if container is already running\n",
        "check_container = subprocess.run(\n",
        "    ['docker', 'ps', '--filter', 'ancestor=nvcr.io/nim/nvidia/nvidia-nemotron-nano-9b-v2:latest', '-q'],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "if check_container.stdout.strip():\n",
        "    print(\"⚠ Container is already running\")\n",
        "    container_id = check_container.stdout.strip().split('\\n')[0]\n",
        "    print(f\"Container ID: {container_id}\")\n",
        "else:\n",
        "    print(\"Starting NIM container...\")\n",
        "    print(\"This will run in detached mode. Monitor logs with: docker logs -f <container_id>\")\n",
        "    \n",
        "    run_result = subprocess.Popen(\n",
        "        [\n",
        "            'docker', 'run', '-d',\n",
        "            '--gpus', 'all',\n",
        "            '--shm-size=16GB',\n",
        "            '-e', f'NGC_API_KEY={NGC_API_KEY}',\n",
        "            '-v', f'{LOCAL_NIM_CACHE}:/opt/nim/.cache',\n",
        "            '-u', str(user_id),\n",
        "            '-p', '8000:8000',\n",
        "            'nvcr.io/nim/nvidia/nvidia-nemotron-nano-9b-v2:latest'\n",
        "        ],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        text=True\n",
        "    )\n",
        "    \n",
        "    stdout, stderr = run_result.communicate()\n",
        "    \n",
        "    if run_result.returncode == 0:\n",
        "        container_id = stdout.strip()\n",
        "        print(f\"✓ Container started successfully\")\n",
        "        print(f\"Container ID: {container_id}\")\n",
        "        print(\"\\nWaiting for model to load (this may take 2-5 minutes)...\")\n",
        "    else:\n",
        "        print(\"✗ Failed to start container\")\n",
        "        print(stderr)\n",
        "        container_id = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Wait for Model to be Ready\n",
        "\n",
        "This cell will poll the health endpoint until the model is ready to accept requests.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Wait for the service to be ready\n",
        "max_retries = 60  # 5 minutes max\n",
        "retry_interval = 5  # seconds\n",
        "\n",
        "print(\"Checking if service is ready...\")\n",
        "for i in range(max_retries):\n",
        "    try:\n",
        "        response = requests.get('http://localhost:8000/v1/models', timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"\\n✓ Service is ready! (took {i * retry_interval} seconds)\")\n",
        "            print(f\"Available models: {response.json()}\")\n",
        "            break\n",
        "    except requests.exceptions.RequestException:\n",
        "        pass\n",
        "    \n",
        "    if i % 6 == 0:  # Print every 30 seconds\n",
        "        print(f\"Waiting... ({i * retry_interval}s elapsed)\")\n",
        "    \n",
        "    time.sleep(retry_interval)\n",
        "else:\n",
        "    print(\"\\n✗ Service did not become ready in time. Check container logs with:\")\n",
        "    print(f\"docker logs {container_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Perform Inference\n",
        "\n",
        "Now let's test the model with a sample question!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform inference\n",
        "url = 'http://localhost:8000/v1/chat/completions'\n",
        "\n",
        "payload = {\n",
        "    \"model\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Which number is larger, 9.11 or 9.8?\"}\n",
        "    ],\n",
        "    \"max_tokens\": 64\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    'accept': 'application/json',\n",
        "    'Content-Type': 'application/json'\n",
        "}\n",
        "\n",
        "print(\"Sending inference request...\\n\")\n",
        "response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(\"✓ Inference successful!\\n\")\n",
        "    print(\"Response:\")\n",
        "    print(json.dumps(result, indent=2))\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Model's Answer:\")\n",
        "    print(result['choices'][0]['message']['content'])\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(f\"✗ Inference failed with status code: {response.status_code}\")\n",
        "    print(response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Try Your Own Questions!\n",
        "\n",
        "Use this cell to experiment with your own prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom inference function\n",
        "def ask_nemotron(question, max_tokens=128):\n",
        "    url = 'http://localhost:8000/v1/chat/completions'\n",
        "    \n",
        "    payload = {\n",
        "        \"model\": \"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ],\n",
        "        \"max_tokens\": max_tokens\n",
        "    }\n",
        "    \n",
        "    response = requests.post(url, json=payload, headers={'Content-Type': 'application/json'})\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result['choices'][0]['message']['content']\n",
        "    else:\n",
        "        return f\"Error: {response.status_code} - {response.text}\"\n",
        "\n",
        "# Try it out!\n",
        "question = \"What are the three laws of robotics?\"\n",
        "answer = ask_nemotron(question)\n",
        "print(f\"Q: {question}\\n\")\n",
        "print(f\"A: {answer}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup (Optional)\n",
        "\n",
        "When you're done, you can stop and remove the container.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop the container\n",
        "if 'container_id' in locals() and container_id:\n",
        "    print(f\"Stopping container {container_id}...\")\n",
        "    subprocess.run(['docker', 'stop', container_id])\n",
        "    print(\"✓ Container stopped\")\n",
        "else:\n",
        "    # Try to find and stop any running nemotron containers\n",
        "    result = subprocess.run(\n",
        "        ['docker', 'ps', '--filter', 'ancestor=nvcr.io/nim/nvidia/nvidia-nemotron-nano-9b-v2:latest', '-q'],\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    if result.stdout.strip():\n",
        "        containers = result.stdout.strip().split('\\n')\n",
        "        for cid in containers:\n",
        "            print(f\"Stopping container {cid}...\")\n",
        "            subprocess.run(['docker', 'stop', cid])\n",
        "        print(\"✓ All containers stopped\")\n",
        "    else:\n",
        "        print(\"No running containers found\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
